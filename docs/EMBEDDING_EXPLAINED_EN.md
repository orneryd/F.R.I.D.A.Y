# Embeddings Simply Explained

## ğŸ¯ The Problem

Computers don't understand words, only numbers!

```
Computer sees:
"Ship" â†’ ??? (What is this?)
"Boat" â†’ ??? (What is this?)
"Car"  â†’ ??? (What is this?)
```

## âœ¨ The Solution: Embeddings

Embeddings convert words into numbers that capture the **meaning**:

```
"Ship" â†’ [0.2, 0.5, 0.3, 0.1, ...]  (384 numbers)
"Boat" â†’ [0.21, 0.48, 0.31, 0.09, ...]  (384 numbers)
"Car"  â†’ [0.8, -0.2, 0.1, 0.5, ...]  (384 numbers)
```

## ğŸ” Why is This Useful?

### Example 1: Recognizing Similarity

```python
# Without embeddings (word comparison):
"Ship" == "Boat"  â†’ False âŒ
"Ship" == "Ship"  â†’ True âœ“

# With embeddings (meaning comparison):
similarity("Ship", "Boat")  â†’ 0.95 (95% similar!) âœ“
similarity("Ship", "Car")   â†’ 0.23 (23% similar)
```

### Example 2: Different Languages

```python
# Without embeddings:
"Ship" == "Schiff"  â†’ False âŒ

# With embeddings:
similarity("Ship", "Schiff")  â†’ 0.98 (98% similar!) âœ“
# Same meaning = Similar vectors!
```

### Example 3: Recognizing Synonyms

```python
# Without embeddings:
"big" == "huge"  â†’ False âŒ

# With embeddings:
similarity("big", "huge")  â†’ 0.87 (87% similar!) âœ“
```

## ğŸ“ How Does It Work?

### Step 1: Training (already done!)

The embedding model was trained on millions of texts:

```
Training data:
"The ship sails on the sea"
"A boat floats in the water"
"The car drives on the road"
...millions more sentences...

â†’ Model learns:
  - "Ship" and "Boat" appear in similar contexts
  - "Ship" and "Sea" belong together
  - "Car" and "Road" belong together
```

### Step 2: Encoding (what we use!)

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# Text â†’ Vector
vector = model.encode("A ship sails on the sea")
# â†’ [0.23, -0.45, 0.67, ..., 0.12]  # 384 numbers
```

### Step 3: Comparing

```python
import numpy as np

# Convert two texts to vectors
v1 = model.encode("A ship")
v2 = model.encode("A boat")

# Calculate cosine-similarity
similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
# â†’ 0.95 (very similar!)
```

## ğŸ¨ Visualization

Imagine each word is a point in 3D space:

```
        Z
        â†‘
        â”‚
        â”‚    â— Ship
        â”‚   â•±â”‚
        â”‚  â•± â”‚â— Boat
        â”‚ â•±  â”‚â•±
        â”‚â•±___â”‚________â†’ X
       â•±     â”‚
      â•±      â”‚
     â•±       â”‚
    â†™        â†“
   Y      â— Car
```

- **Ship** and **Boat** are close together (similar meaning)
- **Car** is far away (different meaning)

**In reality:** 384 dimensions instead of just 3!

## ğŸ”¢ The Numbers in Detail

### What Do the 384 Numbers Mean?

Each dimension captures an aspect of meaning:

```
Dimension 1: "Is it a vehicle?" â†’ 0.8 (yes!)
Dimension 2: "Is it on water?" â†’ 0.9 (yes!)
Dimension 3: "Is it modern?" â†’ 0.3 (maybe)
Dimension 4: "Is it large?" â†’ 0.7 (rather yes)
...
Dimension 384: "Is it historical?" â†’ 0.5 (neutral)
```

**Important:** These dimensions are not defined by humans, but learned by the model!

## ğŸš€ How Friday Uses This

### 1. Training: Storing Knowledge

```
Text: "SMS ZrÃ­nyi was a ship built in 1910"
  â†“ Embedding Model
Vector: [0.23, -0.45, 0.67, ..., 0.12]
  â†“
Create Neuron #42 with this vector
```

### 2. Query: Finding Knowledge

```
Question: "What is a ship?"
  â†“ Embedding Model
Query-Vector: [0.21, -0.43, 0.65, ..., 0.11]
  â†“
Compare with all neurons:
  Neuron #42: Similarity = 0.95 âœ“ (very relevant!)
  Neuron #87: Similarity = 0.89 âœ“ (relevant)
  Neuron #123: Similarity = 0.12 âœ— (not relevant)
  â†“
Use Neurons #42 and #87 for answer
```

## ğŸ’¡ Why is This Better Than Word Search?

### Word Search (old):

```
Question: "What is a ship?"
Search for: "ship"

Finds:
âœ“ "The ship sails..."
âœ— "A boat floats..." (boat â‰  ship)
âœ— "The ship sails..." (ship â‰  ship in German)
```

### Embedding Search (new):

```
Question: "What is a ship?"
Embedding: [0.2, 0.5, 0.3, ...]

Finds:
âœ“ "The ship sails..." (0.98 similar)
âœ“ "A boat floats..." (0.95 similar - synonym!)
âœ“ "Das Schiff fÃ¤hrt..." (0.97 similar - translation!)
```

## ğŸ“ Technical Details

### Cosine-Similarity

How to calculate similarity between two vectors:

```python
def cosine_similarity(v1, v2):
    # Dot product
    dot_product = sum(a * b for a, b in zip(v1, v2))
    
    # Vector lengths
    length_v1 = sqrt(sum(a * a for a in v1))
    length_v2 = sqrt(sum(b * b for b in v2))
    
    # Cosine-Similarity
    return dot_product / (length_v1 * length_v2)
```

**Result:**
- 1.0 = Identical
- 0.9 = Very similar
- 0.5 = Somewhat similar
- 0.0 = Not similar
- -1.0 = Opposite

### Why 384 Dimensions?

```
Fewer dimensions (e.g. 50):
  âœ“ Faster
  âœ— Less precise

More dimensions (e.g. 1024):
  âœ“ More precise
  âœ— Slower
  âœ— More memory

384 dimensions:
  âœ“ Good compromise!
  âœ“ Fast enough
  âœ“ Precise enough
```

## ğŸ”¬ Example Code

### Creating Embeddings

```python
from sentence_transformers import SentenceTransformer

# Load model (once)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Convert texts to vectors
texts = [
    "A ship sails on the sea",
    "A boat floats in the water",
    "A car drives on the road"
]

embeddings = model.encode(texts)
# â†’ 3 vectors with 384 numbers each
```

### Calculating Similarity

```python
from sklearn.metrics.pairwise import cosine_similarity

# Similarity between all texts
similarities = cosine_similarity(embeddings)

print(similarities)
# [[1.00, 0.95, 0.23],   # Ship vs. all
#  [0.95, 1.00, 0.21],   # Boat vs. all
#  [0.23, 0.21, 1.00]]   # Car vs. all
```

### Using in Friday

```python
# 1. Training: Store text
text = "SMS ZrÃ­nyi was a ship"
vector = compression_engine.compress(text)
neuron = KnowledgeNeuron(source_data=text, vector=vector)
graph.add_neuron(neuron)

# 2. Query: Find similar neurons
query = "What is a ship?"
query_vector = compression_engine.compress(query)
results = query_engine.query(query_vector, top_k=5)

# 3. Generate answer
response = generative_model.generate_response(query, results)
```

## ğŸ“š Summary

**Embeddings are:**
- Numerical representations of text
- Capture meaning, not just words
- Enable semantic search
- The key to Friday's intelligence

**Without Embeddings:**
- Only word comparison possible
- No synonyms recognized
- No translations recognized
- No semantic search

**With Embeddings:**
- Meaning comparison possible âœ“
- Synonyms recognized âœ“
- Translations recognized âœ“
- Semantic search âœ“

**The Embedding Model is the foundation of Friday!**

---

**Further Resources:**
- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [How Friday Works](HOW_FRIDAY_WORKS_EN.md)
- [Cosine-Similarity Explained](https://en.wikipedia.org/wiki/Cosine_similarity)
